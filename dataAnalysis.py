from collections import Counter
import pandas as pd
import csv
import pickle
import os
import sys
import numpy as np
import scipy.stats as st
import matplotlib.pyplot as plt
from statistics import stdev, variance, median, mean


# compare results of multiple LDA model with different number of topics
# produce a file with all statistical data about models
# path should be of form <path>/
def compare_results(path, docs_num, start=1):
    # Function returns N largest elements
    def nmaxelements(list1, el):
        final_list = []
        for i in range(0, el):
            max1 = 0
            for j in range(len(list1)):
                if list1[j] > max1:
                    max1 = list1[j]
            list1.remove(max1)
            final_list.append(max1)
        return final_list

    # 15 columns
    rows = [["num_model", "best", "top-2", "top-3", "top-4", "mean", "min", "max", "score-2", "score-3", "score-4", "stdev", "variance", "CI-95", "CI-99"]]
    for x in range(start, docs_num):
        row = [x]
        out = open(path + "data-{}".format(x), "rb")
        data = pickle.load(out)
        out.close()
        values = data['values']
        # best
        best_score_pos = values.index(max(values))
        best_model = data['model-list'][best_score_pos]
        row.append(best_model)
        # top-3
        copy_values = values[:]
        top_3_values = nmaxelements(copy_values, 4)
        second_top = values.index(top_3_values[1])  # second highest
        third_top = values.index(top_3_values[2])  # third highest
        fourth_top = values.index(top_3_values[3])  # fourth highest
        row.append(data['model-list'][second_top])
        row.append(data['model-list'][third_top])
        row.append(data['model-list'][fourth_top])
        mean_value = sum(values)/len(values)
        row.append(mean_value)  # mean
        row.append(min(values))  # min
        row.append(max(values))  # max
        row.append(top_3_values[1])  # top-2 value
        row.append(top_3_values[2])  # top-3 value
        row.append(top_3_values[3])  # top-4 value
        row.append(stdev(values))  #stdev
        row.append(variance(values))  # variance
        # confidence interval
        row.append(st.t.interval(alpha=0.95, df=len(values) - 1, loc=mean_value, scale=st.sem(values)))
        row.append(st.t.interval(alpha=0.99, df=len(values) - 1, loc=mean_value, scale=st.sem(values)))
        rows.append(row)

    with open(path + "results.csv", "w") as f:
        writer = csv.writer(f)
        writer.writerows(rows)


# analyse results from results.csv file generated by previous method
# reports the models distribution for top-1, top-3 and top-4 score
def analyse_top_best_results(file):
    # todo print also other (stdev, var, CIs) data from results.csv
    with open(file, "r") as f:
        reader = csv.reader(f)
        next(reader)  # skip header
        best = []
        best_score = []
        second = []
        second_score = []
        third = []
        third_score = []
        fourth = []
        fourth_score = []
        for row in reader:
            best.append(row[1])
            best_score.append(row[7])
            second.append(row[2])
            second_score.append(row[8])
            third.append(row[3])
            third_score.append(row[9])
            third.append(row[4])
            third_score.append(row[10])
            fourth.append(row[4])
            fourth_score.append(row[11])
        #  top-1 values
        print("top-1 values: ", Counter(best))
        list_int = list(map(float, best_score))
        print("top-1: ", end="")
        print("mean %.5f" % (sum(list_int) / len(list_int)), end=", \t")
        print("stdev %.5f" % stdev(list_int), end=", \t")
        print("variance %.5f" % variance(list_int))
        # top-3 values
        top_3 = best + second + third
        top_3_score = best_score + second_score + third_score
        print("top-3 values: ", Counter(top_3))
        list_int = list(map(float, top_3_score))
        print("top-3: ", end="")
        print("mean %.5f" % (sum(list_int) / len(list_int)), end=", \t")
        print("stdev %.5f" % stdev(list_int), end=", \t")
        print("variance %.5f" % variance(list_int))
        # top-4 values
        top_4 = top_3 + fourth
        top_4_score = top_3_score + fourth_score
        print("top-4 values: ", Counter(top_4))
        list_int = list(map(float, top_4_score))
        print("top-4: ", end="")
        print("mean %.5f" % (sum(list_int)/len(list_int)), end=", \t")
        print("stdev %.5f" % stdev(list_int), end=", \t")
        print("variance %.5f" % variance(list_int), end="\n\n")


# runs analyse_top_best_results for the entire folder
def analyse_top_best_results_folder(path, file):
    folders = []
    for root in os.walk(path):
        if "fix" not in root[0]: folders.append(root[0])
    del folders[0]  # remove first element, since it is the root folder
    folders.sort()
    for folder in folders:
        print("----------- {} ----------".format(folder))
        analyse_top_best_results(folder+"/"+file)


# get data from all results.csv files for all number of documents retrieved
def get_results_file_intra_comparison(path, file_name):
    folders = []
    for root in os.walk(path):
        # only alpha files
        # todo change here for different files to be considered
        if "alpha" not in root[0] and "fix" not in root[0] and "test-" not in root[0]: folders.append(root[0])
    if len(folders) >= 1: del folders[0]  # remove first element, since it is the root folder
    folders.sort()
    print(folders)
    means = []
    stdevs = []
    variances = []
    for x in range(0, len(folders)):
        data_path = folders[x] + "/" + file_name
        mean_values = []
        stdev_values = []
        var_values = []
        with open(data_path, "r") as f:
            reader = csv.reader(f)
            next(reader)  # skip header
            for row in reader:
                mean_values.append(row[5])
                stdev_values.append(row[11])
                var_values.append(row[12])
        means.append(mean_values)
        stdevs.append(stdev_values)
        variances.append(var_values)
    return means, stdevs, variances, folders


# computes the difference between models for various number of topics
def compare_intra_model(path, file_name):
    means, stdevs, variances, folders = get_results_file_intra_comparison(path, file_name)
    for x in range(len(means)):
        # convert from string to float
        mean_values_float = list(map(float, means[x]))
        stdev_values_float = list(map(float, stdevs[x]))
        var_values_float = list(map(float, variances[x]))
        print("-----------------  {}  ----------------".format(folders[x]))
        print("MEAN values")
        print("mean %.5f" % (sum(mean_values_float) / len(mean_values_float)), end=", \t")
        print("stdev %.5f" % stdev(mean_values_float), end=", \t")
        print("variance %.5f" % variance(mean_values_float))

        print("STDEV values")
        print("mean %.5f" % (sum(stdev_values_float) / len(stdev_values_float)), end=", \t")
        print("stdev %.5f" % stdev(stdev_values_float), end=", \t")
        print("variance %.5f" % variance(stdev_values_float))

        print("VARIANCE values")
        print("mean %.5f" % (sum(var_values_float) / len(var_values_float)), end=", \t")
        print("stdev %.5f" % stdev(var_values_float), end=", \t")
        print("variance %.5f" % variance(var_values_float), end="\n\n")


def compare_between_queries_doc_number(path):
    """
    Plot the mean values collected from each query for each docs number.

    :param path: the path to the folder containing queries
    :return:
    """
    num_results = 0
    results = {}
    # must be only from docs_10 to docs_50
    for i in range(5):  # initialize dict
        results["mean_" + str(i)] = []
        results["stdev_" + str(i)] = []
        results["var_" + str(i)] = []
    for query in os.listdir(path):
        print(query)
        if query == "61_5": continue
        if query[0] != "." and query[0] != "d":
            means, stdevs, variances, folders = get_results_file_intra_comparison(path + query, "results.csv")
            if len(means) == 0: continue  # avoid folder without docs_n folders
            num_results += 1
            for i in range(len(means)):
                mean_values_float = list(map(float, means[i]))
                stdev_values_float = list(map(float, stdevs[i]))
                var_values_float = list(map(float, variances[i]))
                results["mean_" + str(i)].append(sum(mean_values_float) / len(mean_values_float))
                results["stdev_" + str(i)].append(sum(stdev_values_float) / len(stdev_values_float))
                results["var_" + str(i)].append(sum(var_values_float) / len(var_values_float))

    data = [results["mean_0"], results["mean_1"], results["mean_2"], results["mean_3"], results["mean_4"]]
    fig7, ax7 = plt.subplots()
    # ax7.set_title('Compare the mean score received for each query for every type of docs')
    ax7.boxplot(data, labels=["10", "20", "30", "40", "50"])

    plt.xlabel("documents retrieved")
    plt.ylabel("mean coherence score")
    plt.savefig("docs-data.png")
    plt.show()
    return results


def compare_between_queries_alpha(path):
    """
    Plot the mean values collected from each query for each docs number.

    :param path: the path to the folder containing queries
    :return:
    """
    num_results = 0
    results = {}
    # must be only from docs_10 to docs_50
    for i in range(8):  # initialize dict
        results["mean_" + str(i)] = []
        results["stdev_" + str(i)] = []
        results["var_" + str(i)] = []
    for query in os.listdir(path):
        print(query)
        if query == "61_5": continue
        if query[0] != "." and query[0] != "d":
            means, stdevs, variances, folders = get_results_file_intra_comparison(path + query, "results.csv")
            if len(means) == 0: continue  # avoid folder without docs_n folders
            num_results += 1
            for i in range(len(means)):
                mean_values_float = list(map(float, means[i]))
                stdev_values_float = list(map(float, stdevs[i]))
                var_values_float = list(map(float, variances[i]))
                results["mean_" + str(i)].append(sum(mean_values_float) / len(mean_values_float))
                results["stdev_" + str(i)].append(sum(stdev_values_float) / len(stdev_values_float))
                results["var_" + str(i)].append(sum(var_values_float) / len(var_values_float))

    data = [results["mean_0"], results["mean_1"], results["mean_2"], results["mean_3"], results["mean_4"],
            results["mean_5"], results["mean_6"], results["mean_7"]]
    fig7, ax7 = plt.subplots()
    ax7.set_title('Compare the mean between different values of alphas')
    ax7.boxplot(data, labels=["1", "2", "3", "4", "5", "6", "7", "8"])
    # ax7.boxplot(data, labels=["0", "1", "10", "20", "40", "60", "80", "100"])
    for d in data:
        print("mean: ", mean(d))
    # plt.xlabel("alpha value")
    plt.xlabel("topic number")
    plt.ylabel("coherence score")
    plt.savefig("fix-data.png")
    # plt.savefig("alpha-data.png")
    plt.show()
    return results


# reads all csv files from provided folder.
# used for single topic files
def analyse_multiple_fix_topic(path, file):
    folders = []
    for root in os.listdir(path):
        if "fix" in root: folders.append(path + root)
    if len(folders) == 0: return
    folders.sort()
    print(folders)
    res = [
        ["doc_name", "best", "top-2", "top-3", "top-4", "mean", "min", "max", "score-2", "score-3", "score-4", "stdev",
         "variance", "CI-95", "CI-99"]]
    for f in folders:
        with open(f + "/" + file, "r") as of:
            reader = csv.reader(of)
            next(reader)  # skip first line
            for row in reader:
                row[0] = f
                res.append(row)  # only one row per document
    with open(path + "fix-results.csv", "w") as f:
        wr = csv.writer(f)
        wr.writerows(res)


def compare_dist(n_docs):
    path = "test/dist/"
    mallet_path = path + "mallet/"
    classic_path = path + "classic/"
    save_path = path + "comparison/"
    # avoid .DS_Store
    dir_list_mallet = os.listdir(mallet_path)
    dir_list_mallet = sorted([el for el in dir_list_mallet if el[0] != "."])
    dir_list_classic = os.listdir(classic_path)
    dir_list_classic = sorted([el for el in dir_list_classic if el[0] != "."])

    assert dir_list_mallet == dir_list_classic

    def collect_data(path, queries):
        dict = {}
        for query in queries:
            list_dir = os.listdir(path + query)
            list_dir = sorted([el for el in list_dir if el[0] != "."])
            for sub in list_dir:
                # todo get min_rel and docs_num
                with open(path + query + "/" + sub + "/results-docs_{}.csv".format(n_docs), "r") as f:
                    reader = csv.reader(f)
                    last_row = None
                    for row in reader:
                        last_row = row
                    # read last row and collect data
                    dict[path + query + "/" + sub] = last_row
        return dict

    dict_classic = collect_data(classic_path, dir_list_classic)
    dict_mallet = collect_data(mallet_path, dir_list_mallet)
    dict = {**dict_classic, **dict_mallet}  # merge to dicts
    # classic string is forward
    header_row = ["topic{}".format(t) for t in range(8)]
    header_row.insert(0, "lda")
    header_row.append("mean")
    header_row.append("n_docs")
    for t in dict:
        cdata = dict[t]
        cdata.insert(0, "classic")
        cdata.append(n_docs)
        classic_string = t.split("/")
        # consider only classic data to avoid repetition
        if classic_string[2] == "mallet":
            continue
        else:
            classic_string[2] = "mallet"
        mallet_string = "/".join(classic_string)
        print(mallet_string)
        mdata = dict[mallet_string]
        mdata.insert(0, "mallet")
        mdata.append(n_docs)

        with open(path + "comparison/" + classic_string[-2] + "_" + classic_string[-1] + ".csv", "w") as f:
            wr = csv.writer(f)
            wr.writerow(header_row)
            wr.writerow(cdata)
            wr.writerow(mdata)


def evaluate_comp(path):
    files = os.listdir(path)
    files = sorted([el for el in files if el[0] != "."])
    dict = {"mallet": 0, "classic": 0, "pair": 0}
    for file in files:
        data = pd.read_csv(path + file)
        names = data["lda"]
        means = data["mean"]
        if means[0] > means[1]:
            dict[names[0]] += 1
        elif means[1] > means[0]:
            dict[names[1]] += 1
        else:
            dict["pair"] += 1
    print(dict)


def create_big_table():
    path = "test/dist/"

    def main(name):
        table = pd.DataFrame()
        for q in os.listdir(path + name):  # q = 31,32,34
            if q[0] != ".":
                for sub in os.listdir(path + name + "/" + q):  # sub = 1,2,3
                    if sub[0] != ".":
                        for files in os.listdir(path + name + "/" + q + "/" + sub):
                            if files[-3:] == "csv":
                                file_path = path + name + "/" + q + "/" + sub + "/" + files
                                with open(file_path, "r") as r:
                                    rd = csv.reader(r)
                                    first = next(rd)
                                    last_line = None
                                    for line in rd:
                                        last_line = line
                                    table = table.append(
                                        pd.Series([name, q, sub, first[0], first[1], first[2]] + last_line),
                                        ignore_index=True)
        return table

    classic_table = main("classic")
    mallet_table = main("mallet")
    final_table = pd.concat([classic_table, mallet_table], ignore_index=True)
    final_table.columns = ["LDA", "caption", "term", "rel_docs", "n_docs", "min_rel"] + ["{}-topics".format(i) for i in
                                                                                         range(8)] + ["mean"]
    final_table.to_csv("test/dist/final.csv")
    print("sas")


def find_num_models_fix(path):
    with open(path + "results.csv", "r") as f:
        reader = csv.reader(f)
        next(reader)
        data = next(reader)
        ci_99 = eval(data[-1])  # convert class string to tuple
        mean_data = float(data[5])

    with open(path + "data-1", "rb") as f:
        data = pickle.load(f)
        scores = data["values"]
        # greater than top
        above_ci_99 = [el for el in scores if el >= ci_99[1]]
        above_mean = [el for el in scores if el >= mean_data]
        print(path)
        print(
            "probability of getting upper bound for ci_99 of {:.4f} is {}/{} = {:.4f} and for the mean of {:.4f} is {}/{} = {:.4f}"
            .format(ci_99[1], len(above_ci_99), len(scores), len(above_ci_99) / len(scores), mean_data, len(above_mean),
                    len(scores), len(above_mean) / len(scores)))


def find_num_models_fix_folder(path):
    for q in os.listdir(path):
        # exclude fix-1 because it is a horizontal line
        if q[0] != "." and "fix" in q and "1" not in q:
            find_num_models_fix(path + q + "/")


if __name__ == '__main__':
    # find_num_models_fix_folder("test/mallet-test/50_1/")
    path = "test/mallet-test/"
    # for q in os.listdir(path):
    #     if q[0] != "." and q[0] != "d" :
    #         analyse_multiple_fix_topic(path+q+"/", "results.csv")
    # sys.exit(0)
    # compare_between_queries_alpha("test/mallet-test/")
    # evaluate_comp("test/dist/comparison/")
    # create_big_table()
    # query = "59_5"
    query = "31_6"
    # analyse_multiple_fix_topic("test/mallet-test/{}/".format(query), "results.csv")
    queries = ["32_1", "32_3", "32_6", "50_1", "50_5", "50_7", "59_2", "59_5", "61_1", "61_8", "67_2", "67_5", "67_10",
               "75_1", "75_3", "75_8"]

    generate = False
    alphas = [0, 10, 20, 40, 60, 80, 100]

    # for a in alphas:
    #     compare_results("test/mallet-test/{}/alpha-{}-docs_30/".format(query, a), 21)
    if generate:
        compare_results("test/mallet-test/{}/alpha-100-docs_30/".format(query), 11)
        compare_results("test/mallet-test/{}/docs_40/".format(query), 21)
        analyse_top_best_results("test/mallet-test/{}/docs_40/results.csv".format(query))

    # analyse_top_best_results_folder("test/mallet-test/{}/".format(query), "results.csv")
    compare_between_queries_doc_number(path)
    # compare_intra_model("test/mallet-test/{}".format(query), "results.csv")

    # analyse_multiple_fix_topic("test/mallet-test/{}/".format(query), "results.csv")

    # for query in queries:
    #     compare_intra_model("test/mallet-test/{}/".format(query), "results.csv")
